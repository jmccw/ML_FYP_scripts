{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ec54dca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt \n",
    "from numpy.linalg import eig\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import clear_output\n",
    "import subprocess\n",
    "import random\n",
    "from datetime import datetime\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6751f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.readlines()\n",
    "\n",
    "    # Extracting layers\n",
    "    layers = []\n",
    "    for line in data:\n",
    "        if line.strip() == 'Layers:':\n",
    "            break\n",
    "    for line in data[data.index('Layers:\\n') + 1:]:\n",
    "        if line.strip() == 'Materials:':\n",
    "            break\n",
    "        layer_info = line.strip().replace(\"(\", \"\").replace(\")\", \"\").replace(\"'\", \"\").split(', ')\n",
    "        layer = (layer_info[0], float(layer_info[1]))\n",
    "        layers.append(layer)\n",
    "\n",
    "    # Extracting materials\n",
    "    materials = {}\n",
    "    for line in data[data.index('Materials:\\n') + 1:]:\n",
    "        if line.strip() == 'QWI Target Shift:':\n",
    "            break\n",
    "        material_info = line.strip().split(': ')\n",
    "        material_name = material_info[0]\n",
    "        properties = [float(prop) for prop in material_info[1][1:-1].split(', ')]\n",
    "        materials[material_name] = properties\n",
    "\n",
    "    # Extracting QWI target shift\n",
    "    qwi_target_shift = float(data[data.index('QWI Target Shift:\\n') + 1])\n",
    "\n",
    "    # Extracting number of electric fields\n",
    "    number_of_electric_fields = int(data[data.index('Number of Electric Fields:\\n') + 1])\n",
    "\n",
    "    # Extracting max applied electric field\n",
    "    max_applied_electric_field = int(data[data.index('Max Applied Electric Field:\\n') + 1])\n",
    "\n",
    "    # Extracting FOM elements\n",
    "    fom_elements = [float(data[data.index('FOM:\\n') + i]) for i in range(1, 5)]\n",
    "\n",
    "    return materials, layers, qwi_target_shift, max_applied_electric_field, fom_elements\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# for files in file_path:\n",
    "#     file_path = 'data.txt'  # Replace 'your_file_path_here.txt' with the actual file path\n",
    "#     materials, layers, qwi_target_shift, max_applied_electric_field, fom_elements = read_data(file_path)\n",
    "#     input_ = [materials, layers, qwi_target_shift, max_applied_electric_field, fom_elements]\n",
    "#     print(input_)\n",
    "#     print(fom_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "25c462e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "input_features_matrix=[]\n",
    "figure_of_merit=[]\n",
    "# Directory path\n",
    "directory = 'Z:/FYP'\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if the file is a text file\n",
    "    if filename.endswith('.txt'):\n",
    "        # Open the file\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        materials, layers, qwi_target_shift, max_applied_electric_field, fom_elements = read_data(file_path)\n",
    "        input_ = [materials, layers, qwi_target_shift, max_applied_electric_field, fom_elements]\n",
    "        input_features_matrix.append(input_)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc097345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'GaAs': [0.111, 1.42, 0.063, 0.082, 0.51, 3.9476],\n",
       "  'GaP': [-0.388, 2.74, 0.25, 0.14, 0.67, 3.3798],\n",
       "  'InP': [0.0, 1.35, 0.077, 0.14, 0.6, 3.3688],\n",
       "  'InAs': [0.441, 0.354, 0.023, 0.026, 0.41, 3.714],\n",
       "  'AlAs': [-0.4245, 2.95, 0.15, 0.16, 0.79, 2.994],\n",
       "  'In0.5314954Ga0.439968Al0.0285366As': [0.30811502529141294,\n",
       "   0.7807065272131057,\n",
       "   0.0442228682,\n",
       "   0.0544621124,\n",
       "   0.464840708,\n",
       "   3.541396534932185],\n",
       "  'In0.528113Ga0.238845Al0.233042As': [0.21488841394518937,\n",
       "   1.0517929225205311,\n",
       "   0.062150133999999996,\n",
       "   0.070602948,\n",
       "   0.52244046,\n",
       "   3.5181736081437682],\n",
       "  'In0.5279419999999999Ga0.228716Al0.243342As': [0.2093955292439803,\n",
       "   1.0677652537249773,\n",
       "   0.063053074,\n",
       "   0.07141592399999999,\n",
       "   0.52534156,\n",
       "   3.516926308226299],\n",
       "  'In0.5313672Ga0.432322Al0.0363108As': [0.3051258622467586,\n",
       "   0.7893984813993644,\n",
       "   0.044904351599999996,\n",
       "   0.0550756792,\n",
       "   0.467030304,\n",
       "   3.5409168008977874],\n",
       "  'In0.527935Ga0.228284Al0.243781As': [0.20915981970038872,\n",
       "   1.0684506551311757,\n",
       "   0.063091547,\n",
       "   0.071450558,\n",
       "   0.5254651800000001,\n",
       "   3.5168754487944067]},\n",
       " [('In0.527935Ga0.228284Al0.243781As', 100.7084940426085),\n",
       "  ('In0.5313672Ga0.432322Al0.0363108As', 252.27785174640593),\n",
       "  ('In0.527935Ga0.228284Al0.243781As', 100.7084940426085)],\n",
       " 5.0,\n",
       " 10,\n",
       " [-0.001058095608190035,\n",
       "  -0.2591805783489617,\n",
       "  0.7998611255891499,\n",
       "  0.3139476126331542]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d47e8774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.001058095608190035, 0.10009279088660716, 2.1896345143733225, 0.20532481225338373]\n",
      "Training data shapes:\n",
      "X_train shape: (998, 23)\n",
      "y_train shape: (998, 4)\n",
      "\n",
      "Testing data shapes:\n",
      "X_test shape: (250, 23)\n",
      "y_test shape: (250, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize lists to store features and targets\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "# Iterate over each heterostructure\n",
    "for heterostructure in input_features_matrix:\n",
    "    # Extract features and targets from the current heterostructure\n",
    "    materials = heterostructure[0]\n",
    "    layers = heterostructure[1]\n",
    "    qwi_target_shift = heterostructure[2]\n",
    "    max_applied_electric_field = heterostructure[3]\n",
    "    fom_elements = heterostructure[4]\n",
    "\n",
    "    # Initialize list to store material features in the correct order\n",
    "    material_features = []\n",
    "\n",
    "    # Iterate over each layer to ensure correct material ordering\n",
    "    for layer in layers:\n",
    "        material_key = layer[0]\n",
    "        if material_key in materials:\n",
    "            material_features.extend(materials[material_key])\n",
    "\n",
    "    # Flatten the layers list\n",
    "    layer_thicknesses = [thickness for _, thickness in layers]\n",
    "\n",
    "    # Combine all features into a single array\n",
    "    all_features = np.concatenate((material_features, layer_thicknesses, [qwi_target_shift], [max_applied_electric_field]))\n",
    "\n",
    "    # Append features to the features list\n",
    "    X_train_list.append(all_features)\n",
    "\n",
    "    # Append targets to the targets list\n",
    "    y_train_list.append(fom_elements)\n",
    "    \n",
    "print(fom_elements)\n",
    "\n",
    "# Print the shape of each element in X_train_list\n",
    "# for i, x in enumerate(X_train_list):\n",
    "#     print(f\"Element {i}: Shape = {x.shape}\")\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "X_train = np.array(X_train_list)\n",
    "y_train = np.array(y_train_list)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training data shapes:\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"\\nTesting data shapes:\")\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b670ed1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shapes:\n",
      "X_train shape: (998, 23)\n",
      "y_train shape: (998, 4)\n",
      "\n",
      "Testing data shapes:\n",
      "X_test shape: (250, 23)\n",
      "y_test shape: (250, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get the maximum shape among all elements in X_train_list\n",
    "max_shape = max(x.shape[0] for x in X_train_list)\n",
    "\n",
    "# Pad the elements to have the same shape\n",
    "X_train_padded = np.array([np.pad(x, (0, max_shape - x.shape[0]), mode='constant') for x in X_train_list])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "y_train = np.array(y_train_list)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_padded, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training data shapes:\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"\\nTesting data shapes:\")\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f3c97897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (fc1): Linear(in_features=23, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        #self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Build neural network model\n",
    "def build_model(input_size):\n",
    "    model = NeuralNetwork(input_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "    return model, criterion, optimizer\n",
    "\n",
    "# Example usage:\n",
    "input_size = X_train.shape[1]  # Input size corresponds to the number of input features\n",
    "model, criterion, optimizer = build_model(input_size)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fe3f26e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000], Loss: 6.0158\n",
      "Epoch [11/2000], Loss: 0.9984\n",
      "Epoch [21/2000], Loss: 0.8784\n",
      "Epoch [31/2000], Loss: 0.8475\n",
      "Epoch [41/2000], Loss: 0.8154\n",
      "Epoch [51/2000], Loss: 0.8001\n",
      "Epoch [61/2000], Loss: 0.7789\n",
      "Epoch [71/2000], Loss: 0.7594\n",
      "Epoch [81/2000], Loss: 0.7344\n",
      "Epoch [91/2000], Loss: 0.7195\n",
      "Epoch [101/2000], Loss: 0.6845\n",
      "Epoch [111/2000], Loss: 0.6150\n",
      "Epoch [121/2000], Loss: 0.5136\n",
      "Epoch [131/2000], Loss: 0.4746\n",
      "Epoch [141/2000], Loss: 0.4724\n",
      "Epoch [151/2000], Loss: 0.4500\n",
      "Epoch [161/2000], Loss: 0.4212\n",
      "Epoch [171/2000], Loss: 0.4178\n",
      "Epoch [181/2000], Loss: 0.4094\n",
      "Epoch [191/2000], Loss: 0.4039\n",
      "Epoch [201/2000], Loss: 0.3799\n",
      "Epoch [211/2000], Loss: 0.3849\n",
      "Epoch [221/2000], Loss: 0.4026\n",
      "Epoch [231/2000], Loss: 0.3880\n",
      "Epoch [241/2000], Loss: 0.3767\n",
      "Epoch [251/2000], Loss: 0.3706\n",
      "Epoch [261/2000], Loss: 0.3756\n",
      "Epoch [271/2000], Loss: 0.3529\n",
      "Epoch [281/2000], Loss: 0.3603\n",
      "Epoch [291/2000], Loss: 0.3544\n",
      "Epoch [301/2000], Loss: 0.3649\n",
      "Epoch [311/2000], Loss: 0.3457\n",
      "Epoch [321/2000], Loss: 0.3593\n",
      "Epoch [331/2000], Loss: 0.3275\n",
      "Epoch [341/2000], Loss: 0.3746\n",
      "Epoch [351/2000], Loss: 0.3406\n",
      "Epoch [361/2000], Loss: 0.3300\n",
      "Epoch [371/2000], Loss: 0.3169\n",
      "Epoch [381/2000], Loss: 0.3236\n",
      "Epoch [391/2000], Loss: 0.3141\n",
      "Epoch [401/2000], Loss: 0.3107\n",
      "Epoch [411/2000], Loss: 0.3135\n",
      "Epoch [421/2000], Loss: 0.3372\n",
      "Epoch [431/2000], Loss: 0.3068\n",
      "Epoch [441/2000], Loss: 0.3191\n",
      "Epoch [451/2000], Loss: 0.3276\n",
      "Epoch [461/2000], Loss: 0.2909\n",
      "Epoch [471/2000], Loss: 0.3426\n",
      "Epoch [481/2000], Loss: 0.3383\n",
      "Epoch [491/2000], Loss: 0.3381\n",
      "Epoch [501/2000], Loss: 0.3032\n",
      "Epoch [511/2000], Loss: 0.2960\n",
      "Epoch [521/2000], Loss: 0.3460\n",
      "Epoch [531/2000], Loss: 0.3449\n",
      "Epoch [541/2000], Loss: 0.2962\n",
      "Epoch [551/2000], Loss: 0.3041\n",
      "Epoch [561/2000], Loss: 0.3177\n",
      "Epoch [571/2000], Loss: 0.3028\n",
      "Epoch [581/2000], Loss: 0.2736\n",
      "Epoch [591/2000], Loss: 0.2655\n",
      "Epoch [601/2000], Loss: 0.3422\n",
      "Epoch [611/2000], Loss: 0.2504\n",
      "Epoch [621/2000], Loss: 0.2850\n",
      "Epoch [631/2000], Loss: 0.2499\n",
      "Epoch [641/2000], Loss: 0.2416\n",
      "Epoch [651/2000], Loss: 0.2419\n",
      "Epoch [661/2000], Loss: 0.2381\n",
      "Epoch [671/2000], Loss: 0.2465\n",
      "Epoch [681/2000], Loss: 0.2342\n",
      "Epoch [691/2000], Loss: 0.2060\n",
      "Epoch [701/2000], Loss: 0.2090\n",
      "Epoch [711/2000], Loss: 0.2156\n",
      "Epoch [721/2000], Loss: 0.1887\n",
      "Epoch [731/2000], Loss: 0.2277\n",
      "Epoch [741/2000], Loss: 0.2101\n",
      "Epoch [751/2000], Loss: 0.2026\n",
      "Epoch [761/2000], Loss: 0.2074\n",
      "Epoch [771/2000], Loss: 0.2251\n",
      "Epoch [781/2000], Loss: 0.2306\n",
      "Epoch [791/2000], Loss: 0.2088\n",
      "Epoch [801/2000], Loss: 0.2018\n",
      "Epoch [811/2000], Loss: 0.1942\n",
      "Epoch [821/2000], Loss: 0.1859\n",
      "Epoch [831/2000], Loss: 0.1776\n",
      "Epoch [841/2000], Loss: 0.1759\n",
      "Epoch [851/2000], Loss: 0.1869\n",
      "Epoch [861/2000], Loss: 0.1752\n",
      "Epoch [871/2000], Loss: 0.2162\n",
      "Epoch [881/2000], Loss: 0.1718\n",
      "Epoch [891/2000], Loss: 0.1745\n",
      "Epoch [901/2000], Loss: 0.1536\n",
      "Epoch [911/2000], Loss: 0.1644\n",
      "Epoch [921/2000], Loss: 0.1563\n",
      "Epoch [931/2000], Loss: 0.1635\n",
      "Epoch [941/2000], Loss: 0.1495\n",
      "Epoch [951/2000], Loss: 0.1838\n",
      "Epoch [961/2000], Loss: 0.1589\n",
      "Epoch [971/2000], Loss: 0.1432\n",
      "Epoch [981/2000], Loss: 0.1816\n",
      "Epoch [991/2000], Loss: 0.1435\n",
      "Epoch [1001/2000], Loss: 0.1337\n",
      "Epoch [1011/2000], Loss: 0.1543\n",
      "Epoch [1021/2000], Loss: 0.3001\n",
      "Epoch [1031/2000], Loss: 0.1597\n",
      "Epoch [1041/2000], Loss: 0.1627\n",
      "Epoch [1051/2000], Loss: 0.1364\n",
      "Epoch [1061/2000], Loss: 0.1355\n",
      "Epoch [1071/2000], Loss: 0.1608\n",
      "Epoch [1081/2000], Loss: 0.1451\n",
      "Epoch [1091/2000], Loss: 0.2874\n",
      "Epoch [1101/2000], Loss: 0.2480\n",
      "Epoch [1111/2000], Loss: 0.2322\n",
      "Epoch [1121/2000], Loss: 0.2231\n",
      "Epoch [1131/2000], Loss: 0.1918\n",
      "Epoch [1141/2000], Loss: 0.1967\n",
      "Epoch [1151/2000], Loss: 0.1976\n",
      "Epoch [1161/2000], Loss: 0.2076\n",
      "Epoch [1171/2000], Loss: 0.1733\n",
      "Epoch [1181/2000], Loss: 0.1852\n",
      "Epoch [1191/2000], Loss: 0.1839\n",
      "Epoch [1201/2000], Loss: 0.1621\n",
      "Epoch [1211/2000], Loss: 0.1955\n",
      "Epoch [1221/2000], Loss: 0.1640\n",
      "Epoch [1231/2000], Loss: 0.1601\n",
      "Epoch [1241/2000], Loss: 0.1984\n",
      "Epoch [1251/2000], Loss: 0.1594\n",
      "Epoch [1261/2000], Loss: 0.1669\n",
      "Epoch [1271/2000], Loss: 0.1843\n",
      "Epoch [1281/2000], Loss: 0.1627\n",
      "Epoch [1291/2000], Loss: 0.1614\n",
      "Epoch [1301/2000], Loss: 0.1471\n",
      "Epoch [1311/2000], Loss: 0.1524\n",
      "Epoch [1321/2000], Loss: 0.1546\n",
      "Epoch [1331/2000], Loss: 0.1538\n",
      "Epoch [1341/2000], Loss: 0.1541\n",
      "Epoch [1351/2000], Loss: 0.1567\n",
      "Epoch [1361/2000], Loss: 0.1898\n",
      "Epoch [1371/2000], Loss: 0.1503\n",
      "Epoch [1381/2000], Loss: 0.1373\n",
      "Epoch [1391/2000], Loss: 0.1382\n",
      "Epoch [1401/2000], Loss: 0.1514\n",
      "Epoch [1411/2000], Loss: 0.1526\n",
      "Epoch [1421/2000], Loss: 0.1446\n",
      "Epoch [1431/2000], Loss: 0.1508\n",
      "Epoch [1441/2000], Loss: 0.1607\n",
      "Epoch [1451/2000], Loss: 0.1576\n",
      "Epoch [1461/2000], Loss: 0.1288\n",
      "Epoch [1471/2000], Loss: 0.1379\n",
      "Epoch [1481/2000], Loss: 0.1405\n",
      "Epoch [1491/2000], Loss: 0.1441\n",
      "Epoch [1501/2000], Loss: 0.1296\n",
      "Epoch [1511/2000], Loss: 0.1386\n",
      "Epoch [1521/2000], Loss: 0.1439\n",
      "Epoch [1531/2000], Loss: 0.1451\n",
      "Epoch [1541/2000], Loss: 0.1273\n",
      "Epoch [1551/2000], Loss: 0.1451\n",
      "Epoch [1561/2000], Loss: 0.1298\n",
      "Epoch [1571/2000], Loss: 0.1220\n",
      "Epoch [1581/2000], Loss: 0.1634\n",
      "Epoch [1591/2000], Loss: 0.1384\n",
      "Epoch [1601/2000], Loss: 0.1203\n",
      "Epoch [1611/2000], Loss: 0.1352\n",
      "Epoch [1621/2000], Loss: 0.1288\n",
      "Epoch [1631/2000], Loss: 0.1389\n",
      "Epoch [1641/2000], Loss: 0.1383\n",
      "Epoch [1651/2000], Loss: 0.1294\n",
      "Epoch [1661/2000], Loss: 0.1213\n",
      "Epoch [1671/2000], Loss: 0.1240\n",
      "Epoch [1681/2000], Loss: 0.1211\n",
      "Epoch [1691/2000], Loss: 0.1123\n",
      "Epoch [1701/2000], Loss: 0.1195\n",
      "Epoch [1711/2000], Loss: 0.1284\n",
      "Epoch [1721/2000], Loss: 0.1339\n",
      "Epoch [1731/2000], Loss: 0.1164\n",
      "Epoch [1741/2000], Loss: 0.1080\n",
      "Epoch [1751/2000], Loss: 0.1164\n",
      "Epoch [1761/2000], Loss: 0.1108\n",
      "Epoch [1771/2000], Loss: 0.1141\n",
      "Epoch [1781/2000], Loss: 0.1179\n",
      "Epoch [1791/2000], Loss: 0.1270\n",
      "Epoch [1801/2000], Loss: 0.1132\n",
      "Epoch [1811/2000], Loss: 0.1220\n",
      "Epoch [1821/2000], Loss: 0.1125\n",
      "Epoch [1831/2000], Loss: 0.1027\n",
      "Epoch [1841/2000], Loss: 0.1112\n",
      "Epoch [1851/2000], Loss: 0.1192\n",
      "Epoch [1861/2000], Loss: 0.1094\n",
      "Epoch [1871/2000], Loss: 0.1050\n",
      "Epoch [1881/2000], Loss: 0.1413\n",
      "Epoch [1891/2000], Loss: 0.1088\n",
      "Epoch [1901/2000], Loss: 0.1058\n",
      "Epoch [1911/2000], Loss: 0.1142\n",
      "Epoch [1921/2000], Loss: 0.1163\n",
      "Epoch [1931/2000], Loss: 0.1140\n",
      "Epoch [1941/2000], Loss: 0.1099\n",
      "Epoch [1951/2000], Loss: 0.1112\n",
      "Epoch [1961/2000], Loss: 0.1103\n",
      "Epoch [1971/2000], Loss: 0.1121\n",
      "Epoch [1981/2000], Loss: 0.1148\n",
      "Epoch [1991/2000], Loss: 0.1134\n",
      "Mean Squared Error (MSE) on test data: 0.0055\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, criterion, optimizer, X_train, y_train, epochs=2000):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in zip(X_train, y_train):\n",
    "            inputs = inputs.clone().detach().type(torch.float32)\n",
    "            targets = targets.clone().detach().type(torch.float32)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            targets = targets.view(-1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(X_train)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = X_test.clone().detach().type(torch.float32)\n",
    "        targets = y_test.clone().detach().type(torch.float32)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        mse = mean_squared_error(targets.numpy(), outputs.numpy())\n",
    "        print(f'Mean Squared Error (MSE) on test data: {mse:.4f}')\n",
    "\n",
    "# Train and evaluate the model\n",
    "def train_and_evaluate_model(model, criterion, optimizer, X_train, y_train, X_test, y_test):\n",
    "    train_model(model, criterion, optimizer, X_train, y_train)\n",
    "    evaluate_model(model, X_test, y_test)\n",
    "\n",
    "# Convert input features and target values to numpy arrays\n",
    "X_train_np = np.array(X_train)\n",
    "y_train_np = np.array(y_train)\n",
    "X_test_np = np.array(X_test)\n",
    "y_test_np = np.array(y_test)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_np, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_np, dtype=torch.float32)\n",
    "\n",
    "# Example usage:\n",
    "epochs = 2000\n",
    "batch_size = 32\n",
    "# Train and evaluate the model\n",
    "train_and_evaluate_model(model, criterion, optimizer, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8591c936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f319198b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.3144e-01, 1.0037e+00, 5.9350e-02, 6.8082e-02, 5.1344e-01, 3.5225e+00,\n",
      "        3.1648e-01, 7.5637e-01, 4.2259e-02, 5.2694e-02, 4.5853e-01, 3.5426e+00,\n",
      "        2.3144e-01, 1.0037e+00, 5.9350e-02, 6.8082e-02, 5.1344e-01, 3.5225e+00,\n",
      "        9.7757e+01, 9.6556e+01, 9.7757e+01, 3.0000e+01, 1.0000e+01])\n",
      "Predicted values: [1.5535699e-03 1.1404514e-01 2.3015819e+00 1.4336526e-01]\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a trained model named 'model'\n",
    "file_path = f\"Z:\\FYP\\data_2024-03-26_14-22-54.txt\"\n",
    "\n",
    "# Read the data from the file\n",
    "materials, layers, qwi_target_shift, max_applied_electric_field, fom_elements = read_data(file_path)\n",
    "\n",
    "# Extract material names from the 'Layers' data in order of appearance\n",
    "material_names_ordered = [layer[0] for layer in layers]\n",
    "\n",
    "# Flatten the materials dictionary in the order of appearance\n",
    "material_features = []\n",
    "for material_key in material_names_ordered:\n",
    "    if material_key in materials:\n",
    "        material_features.extend(materials[material_key])\n",
    "\n",
    "# Flatten the layers list\n",
    "layer_thicknesses = [thickness for _, thickness in layers]\n",
    "\n",
    "# Combine all features into a single array\n",
    "all_features = np.concatenate((material_features, layer_thicknesses, [qwi_target_shift], [max_applied_electric_field]))\n",
    "\n",
    "input_tensor = torch.tensor(all_features, dtype=torch.float32)\n",
    "print(input_tensor)\n",
    "# Pass input data through the model\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    output_tensor = model(input_tensor)\n",
    "\n",
    "# Get the predicted values\n",
    "predicted_values = output_tensor.numpy()\n",
    "\n",
    "print(\"Predicted values:\", predicted_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "517e9437",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x2 and 23x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18512\\53499006.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# Evaluate FOM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mfom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjective_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheterostructure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# Update best FOM and heterostructure if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18512\\53499006.py\u001b[0m in \u001b[0;36mobjective_function\u001b[1;34m(heterostructure)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutput_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18512\\352890364.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x2 and 23x128)"
     ]
    }
   ],
   "source": [
    "def objective_function(heterostructure):\n",
    "    input_tensor = torch.tensor(heterostructure, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        model.eval()  \n",
    "        output_tensor = model(input_tensor)\n",
    "    return output_tensor.numpy()\n",
    "\n",
    "# Define the search space (ranges for each parameter)\n",
    "search_space = {\n",
    "    'material_1_thickness': (0.1, 1.0),\n",
    "    'material_2_thickness': (0.1, 1.0),\n",
    "    # Add more parameters as needed\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "best_fom = -float('inf')\n",
    "best_heterostructure = None\n",
    "\n",
    "for material_1_thickness in np.linspace(search_space['material_1_thickness'][0], search_space['material_1_thickness'][1], num=10):\n",
    "    for material_2_thickness in np.linspace(search_space['material_2_thickness'][0], search_space['material_2_thickness'][1], num=10):\n",
    "        # Create heterostructure\n",
    "        heterostructure = [material_1_thickness, material_2_thickness]  # Add more parameters as needed\n",
    "        \n",
    "        # Evaluate FOM\n",
    "        fom = objective_function(heterostructure)\n",
    "        \n",
    "        # Update best FOM and heterostructure if necessary\n",
    "        if fom > best_fom:\n",
    "            best_fom = fom\n",
    "            best_heterostructure = heterostructure\n",
    "\n",
    "print(\"Best FOM:\", best_fom)\n",
    "print(\"Best Heterostructure:\", best_heterostructure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e175dea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
